{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words, tf-idf\n",
    "\n",
    "Let's investigate one of the most useful feature weightings, and how stop words derive naturally from that. To start, let's load a set of small documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "DATA_DIR = '../../data/'\n",
    "\n",
    "df = pd.read_csv(DATA_DIR + '/rt_critics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.',\n",
       " \"The year's most inventive comedy.\",\n",
       " 'A winning animated feature that has something for everyone on the age spectrum.',\n",
       " \"The film sports a provocative and appealing story that's every bit the equal of this technical achievement.\",\n",
       " \"An entertaining computer-generated, hyperrealist animation feature (1995) that's also in effect a toy catalog.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It seems silly to call such short blurbs 'documents', but we'll stick with the NLP nomenclature.\n",
    "\n",
    "documents = list(df['quote'])\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency\n",
    "\n",
    "Let's start by calculating the document frequency for words in these documents. For this task, let's also remove all the punctuation marks and make everything lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # for tokenizing our text\n",
    "import string  # helps with removing punctuation\n",
    "from collections import Counter  # great dict-like datastructure for counting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a few tokens: [['So', 'ingenious', 'in', 'concept', 'design', 'and', 'execution', 'that', 'you', 'could', 'watch', 'it', 'on', 'a', 'postage', 'stamp', 'sized', 'screen', 'and', 'still', 'be', 'engulfed', 'by', 'its', 'charm'], ['The', 'years', 'most', 'inventive', 'comedy'], ['A', 'winning', 'animated', 'feature', 'that', 'has', 'something', 'for', 'everyone', 'on', 'the', 'age', 'spectrum']]\n",
      "number of tokens: 280092\n",
      "number of unique tokens: 25183\n",
      "number of documents: 14072\n"
     ]
    }
   ],
   "source": [
    "# This is a bit of text cleanup\n",
    "word_bag_list = []\n",
    "for doc in documents:\n",
    "    cleaned = doc.replace('-', ' ')  # make lowercase and split hyphenated words in two\n",
    "    for c in string.punctuation:  # strip punctuation marks.\n",
    "        cleaned = cleaned.replace(c, '')\n",
    "    word_bag_list.append(wordpunct_tokenize(cleaned))\n",
    "\n",
    "# How do things look?\n",
    "print 'a few tokens:', word_bag_list[:3]\n",
    "\n",
    "# this flattens the nested lists into one big list for some stats\n",
    "token_list = []\n",
    "for tokens in word_bag_list:\n",
    "    token_list.extend(tokens)\n",
    "print 'number of tokens:', len(token_list)\n",
    "print 'number of unique tokens:', len(set(token_list))\n",
    "print 'number of documents:', len(word_bag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.5431353041500853),\n",
       " ('and', 0.48493462194428655),\n",
       " ('of', 0.4633314383172257),\n",
       " ('a', 0.4279420125071063),\n",
       " ('is', 0.3306566230812962),\n",
       " ('to', 0.31907333712336555),\n",
       " ('in', 0.22527003979533827),\n",
       " ('that', 0.19783968163729393),\n",
       " ('The', 0.179363274587834),\n",
       " ('it', 0.16742467310972142),\n",
       " ('its', 0.15079590676520752),\n",
       " ('with', 0.14830869812393405),\n",
       " ('but', 0.13331438317225697),\n",
       " ('film', 0.12841102899374646),\n",
       " ('movie', 0.12833996588971006),\n",
       " ('for', 0.1186753837407618),\n",
       " ('as', 0.11725412166003411),\n",
       " ('A', 0.10481807845366686),\n",
       " ('this', 0.10424957362137578),\n",
       " ('an', 0.089681637293917)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the document frequency of all the unique tokens in the bags of words.\n",
    "\n",
    "df = Counter()  # initialize this dict-like thing.\n",
    "\n",
    "for doc in word_bag_list:\n",
    "    # FILL IN CODE\n",
    "    # count up the times words appear in INDIVIDUAL documents (not the total across all documents)\n",
    "    for token in set(doc):\n",
    "        df[token] += 1\n",
    "\n",
    "# normalize the counts by the number of documents (are you getting zeros? Think datatypes.)\n",
    "for token in df:\n",
    "    df[token] = df[token] / float(len(documents))\n",
    "\n",
    "# this prints the 20 highest-scoring words and their scores\n",
    "df.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Which words are likely to be stop words? The ones that show up in the most documents! These terms with the largest document frequency are the stopwords! The threshold above which you call something a stopword is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "More interesting than stop-words is the tf-idf score. This tells us which words are most discriminative between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf} = tf \\cdot -\\log{df}\n",
    "$$\n",
    "\n",
    "recall that:\n",
    "\n",
    "$$\n",
    "\\log{x} = -\\log{1 \\over x}\n",
    "$$\n",
    "\n",
    "What are the most discriminative words in the first few documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate the term frequency of all the unique tokens in all of the bags of words.\n",
    "\n",
    "for doc in word_bag_list[:100]:\n",
    "    tf = Counter()  # initialize this dict-like thing.\n",
    "    tfidf = Counter()\n",
    "    \n",
    "    # FILL IN CODE\n",
    "\n",
    "    # calculate term frequencies\n",
    "    for token in doc:\n",
    "        tf[token] += 1\n",
    "    total = float(sum(tf.values()))\n",
    "\n",
    "    # calculate tf-idf scores\n",
    "    for token in tf:\n",
    "        tfidf[token] = (tf[token] / total) * (-np.log(df[token]))\n",
    "\n",
    "    # this prints most significant words in the document\n",
    "    print tfidf.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn\n",
    "\n",
    "Scikit-Learn comes with utilities to do these calculations for us. How do their results compare?\n",
    "\n",
    "I confess, I ran out of time to do a proper comparison, but with enough work, we can figure out which features (i.e. words) have the highest scores. What's happening is each documents is converted into a normalized vector (length = 1) where most of the dimensions/features/words are 0, and the words that occur in the document get a score proportional to its tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.25400101  0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "output = tfidf_vec.fit_transform(documents)\n",
    "print output.toarray()[20:30, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print tfidf_vec.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".text_cell_render {\n",
       "  background-color: cyan;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x10ba2ea90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<style>\n",
    ".text_cell_render {\n",
    "  background-color: cyan;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
